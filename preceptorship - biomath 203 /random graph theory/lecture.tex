\documentclass[./some_latex_template.tex]{subfiles}
\begin{document}

\title{Random Graph theory}
\author{Benjamin Chu}
\maketitle

\singlespacing

\section{Basics of Graph Theory}

\begin{itemize}
	\item A graph $G$ is a pair of sets $G = (V, E)$ where $V$ is a set of vertices and $E$ is a set of edges where $e \in E$ can be written as $e = (x, y)$ where $x, y \in V$. 
	\item It is comon to represent a graph by \textit{drawing}. Each vertex $v \in V$ is represented as a point in the plane, while edges are lines connecting pairs of points. 
\end{itemize}

\noindent There are a number of special graphs, which we will only mention. 
\begin{itemize}
	\item A graph with $n$ nodes is \textbf{complete} (denoted by $K_n$) if every node forms an edge with every other node. 
	\item A \textbf{cycle graph} (denoted by $C_n$) is a graph that consists of nodes connected in a closed chain. The degree of each vertex is 2. 
	\item A \textbf{tree} is a connected graph with no cycles. 
\end{itemize}

\noindent The following theorem will get you started with the basics of graph theory.

\begin{theorembox}{First theorem of graph theory}{}
A finite graph $G$ has an even number of vertices with odd \textbf{degree} (i.e. the number of edges incident to it).
\end{theorembox}

\begin{proof}
Since each edge connects 2 nodes, 
\begin{align*}
	2|E| = 	\sum_{v \in V} deg(v) = \sum_{\substack{v \in V\\ deg(v) \text{ even}}} deg(v) + \sum_{\substack{u \in V\\ deg(u) \text{ odd}}} deg(u) \ \Longrightarrow \ \left(\sum_{\substack{u \in V\\ deg(u) \text{ odd}}} deg(u)\right) \text{ is even.}
\end{align*}
If the sum is even, and each summand is odd, then there must be an even number of summands.
\end{proof}

\section{Sharp Threshold for Connectivity in Erdos-Renyi Graph Model}

Most materials for this section note is taken from \cite{Acemoglu, Ramchandran}. First some background:

\begin{itemize}
	\item We use $G(n, p)$ to denote an undirected (Erdos-Renyi) graph with $n$ nodes and probability of forming an edge $p(n)$.
	\item Each edge forms with probability $p \in (0, 1)$ \textbf{independently} of other edges. 
	\item An graph is \textbf{connected} if there is a path between any 2 pairs of nodes. 
	\item When $p = p(n)$ is a function of $n$, we may be interested in the behavior of $G(n, p(n))$ as $n \rightarrow \infty$. 
\end{itemize}

\subsection{Warm-up}

\textbf{Q1. What is the probability that a vertex is isolated in $G(n, p)$?} \textbf{Ans:} A given node $i$ cannot form an edge with each of the remaining $n - 1$ nodes. Thus the probability is $(1 - p)^{n-1}$. 

\noindent \textbf{Q2. What is the probability that node 1 and node 2 are both isolated?} \textbf{Ans:} Let $I_1, I_2$ be the indicator that node 1 and node 2 are isolated. Then $P(I_1 \cap I_2) = P(I_1)P(I_2 \ | \ I_1)  = (1-p)^{n-1} * (1 - p)^{n - 2} = (1 - p)^{2n-3}$. 

\noindent \textbf{Q3. What is the probability that a group of $k$ nodes do not connect to the rest of the $n-k$ nodes?} \textbf{Ans:} There are ${n \choose k}$ number of ways to choose $k$ vertices. Each of these cannot form an edge with the remaining $n-k$ nodes independently with probability $(1-p)^{n-k}$. So overall we have $(1-p)^{(n-k)k}$. \\

\begin{theorembox}{Erdos-Renyi 1961}{}
Consider a graph $g \sim G(n, p(n))$ where $p(n) = \lambda \frac{\ln(n)}{n}$. Then as $n \rightarrow \infty$,
\begin{align*}
	P(g \text{ connected}) \rightarrow 0 & \quad \text{if } \lambda < 1\\
	P(g \text{ connected}) \rightarrow 1 & \quad \text{if } \lambda > 1
\end{align*}
\end{theorembox}

\begin{proof}
Suppose $\lambda < 1$. Since $P(\text{connected}) = 1 - P(\text{disconnected})$, we will show $P(\text{disconnected}) \rightarrow 1$ by showing that \textbf{there is at least 1 isolated node}. Define
\begin{itemize}
	\item $X_n$ to be a random variable that counts the number of isolated nodes
	\item $I_i$ to be a (Bernoulli) indicator random variable such that $I_i = 1$ when node $i$ is isolated and is $0$ otherwise
	\item Let $p = p(n)$ and $q = q(n) = (1 - p(n))^{n - 1}$ be  the probability of a node being isolated
\end{itemize}
We want to show $P(X_n > 0) \rightarrow 1$, or equivalently, $P(X_n = 0) \rightarrow 0$. To get a bound on $P(X_n = 0)$, we observe:
\begin{align*}
	\Var(X_n) 
	&= E\left(X_n - E(X_n)\right)\\
	&= P(X_n = 0)(0 - E(X_n)^2 + P(X_n = 1)(1 - E(X_n))^2 + ...\\
	&\ge P(X_n = 0)E(X_n)^2.
\end{align*}
Thus 
\begin{align}\label{claim1}
\frac{\Var(X_n)}{E(X_n)^2} \ge P(X_n=0). 
\end{align}
We will now calculate $\Var(X_n)$ and $E(X_n)$ explicitly to show that the left hand side of \eqref{claim1} goes to 0. By linearity of expectation and applying definition of indicators,
\begin{align*}
	E(X_n) = E\left(\sum_{i=1}^n I_i\right) = \sum_{i=1}^nE(I_i) = \sum_{i=1}^nP(I_i) = nq.
\end{align*}
Since indicators $I_i$ are \textbf{not independent} (why?), we use equation (1.10) in your book \cite{lange2010applied}:
\begin{align*}
	\Var(X_n) 
	&= \Var\left(\sum_{i=1}^nI_i\right) = \sum_{i=1}^n\Var(I_i) + \sum_{i=1}^n\sum_{j\neq i} \Cov(I_i, I_j)\\
	&= \sum_{i=1}^n q(1-q) + \sum_{i=1}^n \sum_{j\neq i}\left[ E(I_iI_j) - E(I_i)E(I_j) \right] \quad (\text{since Var(Bernoulli)} = p(1-p))\\
	&= nq(1-q) + \sum_{i=1}^n \sum_{j\neq i}\left[ P(I_i \cap I_j) - P(I_i)P(I_j) \right]\\
	&= nq(1-q) + \sum_{i=1}^n\sum_{j\neq i}\left[ (1-p)^{n-1}(1-p)^{n-2} - (1-p)^{n-1}(1-p)^{n-1} \right]\\
	&= nq(1-q) + \sum_{i=1}^n\sum_{j\neq i}\left[ \frac{q^2}{1-p} - q^2 \right]\\
	&= nq(1-q) + n(n-1)q^2\frac{p}{1-p}.
\end{align*}
Thus 
\begin{align*}
	\frac{\Var(X_n)}{E(X_n)^2} 
	&= \frac{nq(1-q) + n(n-1)q^2\frac{p}{1-p}}{(nq)^2} = \frac{1-q}{nq} + \frac{n-1}{n}\frac{p}{1-p}.
\end{align*}
We will now show these terms approach 0 as $n \rightarrow \infty$, then eq \eqref{claim1} will give us what we need.
The first term is dominated by $nq$, and
\begin{align*}
	\lim_{n \rightarrow \infty} nq 
	&= \lim_{n \rightarrow \infty} n(1-p)^{n-1} = \lim_{n \rightarrow \infty}  \exp \left\{ \ln(n) + (n-1)\ln(1 - p) \right\}\\
	&= \lim_{n \rightarrow \infty} \exp \left\{ \ln(n) + (n-1)\ln\left( 1 - \frac{\lambda \ln(n)}{n} \right) \right\}\\
	&\approx \lim_{n \rightarrow \infty} \exp \left\{ \ln(n) - \lambda\frac{n-1}{n}\ln(n) \right\} \quad (\ln(1 - x) = 1 - x + \frac{x^2}{2}- ... \approx -x + O(x^2) \text{ for small }x)\\
	&= \lim_{n \rightarrow \infty} \exp \left\{ \ln(n)\left( 1 - \lambda\frac{n-1}{n} \right) \right\}\\
	&= \infty \qquad \left(\text{since } \lambda < 1 \text{ and } n \rightarrow \infty \right)
\end{align*}
For the second term, observe that $p = \lambda \frac{\ln(n)}{n} \rightarrow 0$ as $n \rightarrow \infty$. So $\frac{p}{1-p} \rightarrow 0$ as well. This completes the case for $\lambda < 1$. \\
\\
\textbf{Part II.} Now suppose $\lambda > 1$. We want to show $P(\text{connected})\rightarrow 1,$ or equivalently $P(\text{disconnected})\rightarrow 0$. A graph is disconnected if there is a subgraph of $k$ nodes that does not connect to any of the other $n - k$ nodes (draw a picture). By symmetry, we only have to consider $k \in \{1, 2, ... \lfloor n/2 \rfloor\}$.  So 
\begin{align*}
	P(\text{disconnected}) 
	&= \bigcup_{k=1}^{\lfloor n/2 \rfloor} P(\text{some set of } k \text{ nodes not connected to the rest})\\
	&\le \sum_{k=1}^{\lfloor n/2 \rfloor} P(\text{some set of } k \text{ nodes not connected to the rest}) \quad (\text{inclusion-exclusion picture})\\
	&= \sum_{k=1}^{\lfloor n/2 \rfloor} {n \choose k} \left[ (1-p)^{(n - k)} \right]^k\\
	&\le \sum_{k=1}^{\lfloor n/2 \rfloor} {n \choose k} e^{p(n-k)k} \quad \left(e^{-x} = 1 - x + \frac{x^2}{2} - ... \approx 1 - x + O(x^2) \text{ for small }x\right)\\
	&= \sum_{k=1}^{\lfloor n/2 \rfloor} {n \choose k} \exp\left\{\frac{-\lambda \ln(n)(n-k)k}{n}\right\}\\
	&= \sum_{k=1}^{\lfloor n/2 \rfloor} {n \choose k} n^{\frac{-\lambda}{n}(n-k)k}\\
	&= \sum_{k=1}^{n^*} {n \choose k} n^{\frac{-\lambda}{n}(n-k)k} + \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} {n \choose k} n^{\frac{-\lambda}{n}(n-k)k} \quad \left(\text{Choose } n^* s.t. \frac{\lambda(n - n^*)}{n}>1 \iff n^* = \lfloor n(1 - \frac{1}{\lambda}) \rfloor\right)
\end{align*}
For the first term, 
\begin{align*}
	\sum_{k=1}^{n^*} {n \choose k} n^{\frac{-\lambda}{n}(n-k)k}
	&\le \sum_{k=1}^{n^*} n^k n^{\frac{-\lambda}{n}(n-k)k} = \sum_{k=1}^{n^*} \left[ n^{1 - \frac{\lambda}{n}(n-k)} \right]^k\\
	&\le \sum_{k=1}^{n^*} \left[ n^{1 - \frac{\lambda}{n}(n-n^*)} \right]^k \qquad \left(\text{judiciously bound inner} k \text{ with something bigger}\right)\\
	&= \sum_{k=1}^{n^*} r^k \qquad \left(\text{define } r = n^{1 - \frac{\lambda}{n}(n-n^*)}\right)\\
	&= \left(\sum_{k=0}^{n^*} r^k\right) - 1\\
	&= \frac{r}{1-r} \qquad (\text{geometric series; } 1 - \frac{\lambda}{n}(n-n^*) < 0, \text{ so } r < 1)\\
	&= \frac{1}{n^{\frac{\lambda}{n}(n-n^*) - 1} - 1}\\
	&\longrightarrow 0 \qquad \left(\text{since } n \rightarrow \infty \text{ and exponent} > 0\right)
\end{align*}
For the second term, we use a better bound than before (see homework):
\begin{align*}
	{n \choose k} < \left(\frac{ek}{k}\right)^k.
\end{align*}
Thus 
\begin{align*}
	\sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} {n \choose k} n^{\frac{-\lambda}{n}(n-k)k} 
	&\le \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} \left(\frac{en}{k}\right)^k n^{\frac{-\lambda(n-k)k}{n}} = \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} \left[ \frac{en^{1 - \frac{\lambda(n-k)}{n}}}{k}\right]^k\\
	&\le \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} \left[ \frac{en^{1-\frac{\lambda (n - \frac{n}{2})}{n}}}{n^* + 1}\right]^k \qquad (\text{bound inner } k \text{ with something from above})\\
	&= \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} \left[\frac{en^{1 - \frac{\lambda}{2}}}{n(1 - \frac{1}{\lambda}) + 1}\right]^k \le \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor} \left[\frac{en^{\frac{-\lambda}{2}}}{1 - \frac{1}{\lambda}}\right]^k\\
	&\le \sum_{k=n^* + 1}^{\lfloor n/2 \rfloor}  r^k \qquad (r = \frac{en^{\frac{-\lambda}{2}}}{1 - \frac{1}{\lambda}}, 0 < r < 1 \text{ for large }n)\\
	&\le \sum_{k=n^* + 1}^{\infty}r^k  = \sum_{k=0}^{\infty}r^k - \sum_{k=n^* + 1}^{n^*}r^k\\
	&= \frac{1}{1-r} - \frac{1-r^{n^*+1}}{1-r} \qquad (\text{finite geometric series} \sum_{k=0}^{m}r^k = \frac{1-r^{m+1}}{1-r})\\
	&= \frac{r^{n^* + 1}}{1-r} \longrightarrow 0 \qquad \text{since } n^* \rightarrow \infty.
\end{align*}
\end{proof}

\section{Clustering graphs}

Sometimes it is useful to \textbf{cluster} a graph, which lumps a graph's nodes into several groups so that there are much more edges within groups than between groups. There are many algorithms to do this (e.g. K-means, hierarchical), which is not our focus. Rather, we will combine random graph theory with (discrete time) Markov chains to define a distance metric that can be used together with various clustering algorithms. Most material is based on \cite{yen2005clustering}. 

%\noindent Motivations for graph clustering::
%
%\begin{itemize}
%	\item Document classification
%	\item Image segmentation
%	\item Protein structural integrity: consider a protein where each amino acid residue are nodes. We can cluster the residues to identify protein substructure and/or important residues crucial for the protein's structural or catalytic functions.  
%\end{itemize}

\subsection{Euclidean distance for clustering in k-means algorithm}

Clustering algorithms require some measures of similarity (i.e. distance) between 2 nodes. One common distance measure is the Euclidean distance: $d(\bx, \by) = \sqrt{\sum_{i}(x_i - y_i)^2}$. This defines the traditional k-means and hierarchical clustering algorithms.\\
\\
\textbf{(review) k-means algorithm:}
\begin{enumerate}
	\item Randomly initialize $k$ cluster centers $c_1,...c_k$. 
	\item Assign each node to some cluster based on the smallest \textbf{Euclidean distance} to each cluster center $c_i$.
	\item Update center location $c_1, ..., c_k$ by computing the means of the nodes in the cluster. 
	\item Repeat 2 and 3 until assignments no longer change
\end{enumerate}

It is well known that k-means often fail because the Euclidean distance implicitly assumes data has Gaussian shape. Some examples that it fails:

\begin{figure}[H]
  \begin{subfigure}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{figures/shape.png}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figures/shape2.png}
  \end{subfigure}
\end{figure}

Figures from
\begin{itemize}
	\footnotesize
	\item \texttt{https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means}
	\item \texttt{http://i-systems.github.io/HSE545/machine\%20learning\%20all/05\%20Clustering/iSystems\_01\_K-means\_Clustering.html}
\end{itemize}

\subsection{Random Walk on a Graph is a Markov chain}

Markov chain setup:
\begin{itemize}
	\item Consider a connected \textbf{weighted graph} with $N$ nodes where each edge connecting nodes $i$ and $j$ has a weight $w_{ij} > 0$ that is symmetric $w_{ij} = w_{ji}$. 
	\item Let each node of a graph be a state in a Markov chain. 
	\item For node $i$, the probability of jumping to an adjacent node $j$ is $p_{ij} = \frac{a_{ij}}{a_{i\cdot}}$ where $a_{i\cdot} = \sum_{j} a_{ij}.$ Hence large $w_{ij}$ values means easier communication through the edge. 
	\item Connectivity implies the Markov chain is irreducible. 
\end{itemize}

\noindent Based on this Markov chain, we define 3 important quantities: 

\begin{itemize}
	\item Starting at state $i$, the \textbf{average first passage time} \begin{align*}		
		\begin{cases}
			m(k|k) = 0 \\
			m(k|i) = 1 + \sum_{j=1}^N p_{ij}m(k|j) & \text{if } i\neq k.
		\end{cases}
	\end{align*}
	is the average number of steps a random walker needs to enter state $k$.
	\item Starting at state $i$, the \textbf{average first passage cost} \begin{align*}		
		\begin{cases}
			o(k|k) = 0 \\
			o(k|i) = \sum_{j=1}^N p_{ij}c(j|i) + \sum_{j=1}^N p_{ij}m(k|j) & \text{if } i\neq k.
		\end{cases}
	\end{align*}
	is the average cost incurred by the random walker to enter state $k$. $c(j|i)$ is the cost of transitioning from $i$ to $j$. Note $m(k|i)$ is a special case of $o(k|i)$ where all $c(j|i) = 1$. 
	\item Starting at $i$, the \textbf{average commute time} 
	\begin{align*}
		n(i,j) = m(j|i) + m(i|j)
	\end{align*}
	is the number of steps a random walker take to enter $j \neq i$ for the first time, then go back to $i$. One can show that this is a metric. 
\end{itemize}

\noindent \textbf{Intuition:} $n(i, j)$ decreases when 2 nodes are highly connected, or when the length of any path decrease. The fact that it takes "connectivity" between nodes into account sets it apart from  shortest path distances.

\subsection{Euclidean Commute Time (ETC) Distance}

Define the \textbf{adjacency matrix} $\bA = (a_{ij})$ where $a_{ij} = w_{ij}$ if node $i$ is connected to $j$, otherwise $a_{ij} = 0$. Also define $\bD = \text{diag}(a_{i\cdot})$  where $a_{i\cdot} = \sum_{j}a_{ij}$. The \textbf{Laplacian matrix} of the graph is $\bL = \bD - \bA$, which is not full rank because $\bone$ (vector of 1s) is in its null space. Hence the following theorem involves a pseudoinverse $\bL^+$: 

\begin{theorembox}{Computaton of average commute time $n(i, j)$}{}
We can compute average commute time between nodes $i$ and $j$ by:
\begin{align*}
	n(i, j) = V_G (\be_i - \be_j)^t\bL^+(\be_i - \be_j)
\end{align*}
where $\bL^+$ is the Moore-Penrose pseudoinverse of $\bL, V_G = \sum_{i,j} a_{ij}$ is the volume of the graph, and $\be_i$'s are the standard basis vectors that is $0$ everywhere and is $1$ at position $i$. 
\end{theorembox}
\begin{proof}
See appendix of \cite{fouss2007random}. 
\end{proof}

\noindent Observe that:
\begin{enumerate}
	\item $\bL^+$ is symmetric since $\bL$ is. 
	\item $\bL^+$ is positive semidefinite, since $\bL$ psd $\iff \bL^+$ psd, and $\bL$ is psd since it is diagonally dominant. 
\end{enumerate}

\noindent Here (1) + (2) above implies that $\bL^+$ defines an inner product between $\bx$ and $\by$ as $\langle\bx, \by\rangle = \bx^t\bL^+\by$ in $\R^n$. This induces a norm: $||\bx|| = (\bx^t\bL^+\bx)^{1/2}$. Therefore, the quantity $[n(i, j)]^{1/2}$ is called the \textbf{Euclidean Commute Time} (ETC) \textbf{Distance}. 

\subsection{Revised k-means clustering using ETC Distance}

\begin{enumerate}
	\item Connect each node to its $h$ nearest neighbors, then add the edges of a minimum spanning tree. Then add weights $w_{ij}$ using your favorite method (e.g. inverse Euclidean distance). 
	\item Choose number of clusters $k$ and its respective cluster centers $\bp_1,...,\bp_k$ where each $\bp_i$ is a node.
	\item Assign each node $\bx_i$ to the nearest cluster $C_l$ by finding which of $\bp_1,...,\bp_k$ is closest to $\bx_i$, where closeness is measured by the ETC distance: $\dist(\bx_i, \bp_j) = n(i, j)^2$ 
	\item Recompute new cluster centers $\bp_1,...,\bp_k$ so by minimizing the within-cluster distance:
	\begin{align*}
		\bp_l = \argmin_{\bx_j} \left\{ \sum_{\bx_k \in C_l}n(k, j)^2 \right\}
	\end{align*}
	\item Repeat (2) and (3) until convergence. 
\end{enumerate}

\noindent This gives the following clustering result:

\begin{figure}[H]
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/etc_1.png}
  \end{subfigure}
  \quad 
  \begin{subfigure}[b]{0.4\textwidth}
    \includegraphics[width=\textwidth]{figures/etc_2.png}
  \end{subfigure}
\end{figure}

\section{Problems}

In honor of Ken Lange, you are required to do 2 problems. If you do more I will grade your top 2 problems. Every problem is worth the same number of points. If a problem has subproblems, each subproblem is worth the same number of points.

\begin{problembox}{Bounds of binomial coefficients}{}
For integers $n$ and $k$, prove the following inequalities
\begin{align*}
	\frac{n^k}{k^k} \le {n \choose k} \le \frac{n^k}{k!} < \left( \frac{ne}{k}\right)^k
\end{align*}
which is used in part 2 of our sharp threshold proof. For the strict inequality, rewrite $\frac{n^k}{k!} = \left(\frac{n}{k}\right)^k \frac{k^k}{k!}$ and use Taylor expansion on $e^k$.
\end{problembox}

\begin{problembox}{}{}
Prove that $\bL^+$ in theorem 3.1 can be computed via
\begin{align*}
	\bL^+ = \left(\bL - \frac{ \bone\bone^t }{n}\right)^{-1} + \frac{\bone\bone^t}{n}
\end{align*}
where $\bone$ is a vector of 1s. This is equation (3) in \cite{fouss2007random}, but it came without a proof.
\end{problembox}

\begin{problembox}{Colorings of graphs}{}
Let $K_z$ be a \textbf{complete graph} where all $z \in \Z_+$ nodes forms an edge with every other node. With equal probability, each edge is colored with red or green. Prove that $z=6$ is the minimal number of nodes needed to guarantee the existance of a \textbf{monochromatic triangle} (i.e. triangle with all edges the same color). This type of problem is what Ramsey theory studies, which we almost did. 
\end{problembox}
In Ramsey theory, this corresponds to the value $R(3, 3)$. Similarly, $R(3, 4)$ is the minimal number of nodes to guarantee a red triangle or green square. Function $R$ obviously generalizes to more colors and shapes. Using Erdos' probabilistic method, Ramsey's theorem (see \cite{sebv} or theorem 3.3 of \cite{van2001course}) says this number is finite but exponential.  This takes us to Erdos' famous quote:

\begin{displayquote}
Suppose aliens invade earth and threaten to obliterate us within a year unless human beings can find $R(5, 5)$. We could marshal the world's best minds and fastest computers, and within a year we could probably calculate the value. However, if the aliens demanded $R(6, 6)$, we would have no choice but to launch a preemptive attack.
\end{displayquote}
If you want to be famous, find $R(5, 5)$. 
\bibliographystyle{apalike}
\bibliography{references}

\end{document}